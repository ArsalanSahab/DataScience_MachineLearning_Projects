{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-7-3f75c9656201>, line 2)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-3f75c9656201>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    import pandas as pd\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import cv2\n",
    "import os\n",
    "import pywt\n",
    "import shutil\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-2-2f947c8480cb>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-2f947c8480cb>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    test_img = cv2.imread('../Datasets/lionel_messi/_111066400_messi.jpg')\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "# Examining a sample Image \n",
    "\n",
    "test_img = cv2.imread('../Datasets/lionel_messi/_111066400_messi.jpg')\n",
    "test_img.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lets see the image without the RGB Values \n",
    "\n",
    "gray_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2GRAY)\n",
    "gray_img.shape\n",
    "\n",
    "plt.imshow(gray_img, cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "To clean the data i need to :\n",
    "\n",
    "1. Crop Images.\n",
    "2. Check if an image has a FACE and TWO EYES (Using Harcascade)\n",
    "3. Check if the image is of the celebrity i want.\n",
    "4. Check for noise in the images\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Diffrent Cascade Methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising Cascade Models\n",
    "\n",
    "face_cascader = cv2.CascadeClassifier('../OpenCV_Dependencies/haarcascades/haarcascade_frontalface_default.xml')\n",
    "eye_cascader = cv2.CascadeClassifier('../OpenCV_Dependencies/haarcascades/haarcascade_eye.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Face Cascade Model to Image\n",
    "\n",
    "face_detector = face_cascader.detectMultiScale(gray_img, 1.3, 5)\n",
    "face_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 4 Values represent : X, Y , Widht and Height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Plotting the Detected Face \n",
    "\n",
    "(x_cord, y_cord, width, height) = face_detector[0]\n",
    "\n",
    "face_detected_img = cv2.rectangle(test_img, (x_cord, y_cord), (x_cord+width, y_cord+height), (0, 255, 0), 2)\n",
    "plt.imshow(face_detected_img) \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Applying Eye Cascade + Face Cascade Model to Image\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "for ((x_cord, y_cord, width, height)) in face_detector :\n",
    "\n",
    "    face_detected_img = cv2.rectangle(test_img, (x_cord, y_cord), (x_cord+width, y_cord+height), (0, 255, 0), 2)\n",
    "\n",
    "    roi_gray = gray_img[y_cord:y_cord+height, x_cord:x_cord+width]\n",
    "    roi_color = face_detected_img[y_cord:y_cord+height, x_cord:x_cord+width]\n",
    "\n",
    "    eye_detector = eye_cascader.detectMultiScale(roi_gray)\n",
    "\n",
    "    for ((eye_x_cord, eye_y_cord, eye_width, eye_height)) in eye_detector :\n",
    "\n",
    "\n",
    "        cv2.rectangle(roi_color, (eye_x_cord, eye_y_cord), (eye_x_cord+eye_width, eye_y_cord+eye_height), (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(face_detected_img, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Cropped Image with Face and Eyes Detected\n",
    "plt.imshow(roi_color) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Lets Create a Function so that we can apply later\n",
    "\n",
    "def image_detector_cropper(path_to_image) :\n",
    "\n",
    "        image = cv2.imread(path_to_image)\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        face_detector = face_cascader.detectMultiScale(gray_image, 1.3, 5)\n",
    "        \n",
    "        \n",
    "        for ((x_cord, y_cord, width, height)) in face_detector :\n",
    "\n",
    "                roi_gray = gray_image[y_cord:y_cord+height, x_cord:x_cord+width]\n",
    "                roi_color = image[y_cord:y_cord+height, x_cord:x_cord+width]\n",
    "                eyes_detector = eye_cascader.detectMultiScale(roi_gray)\n",
    "\n",
    "                if (len(eyes_detector) >= 2):\n",
    "\n",
    "                    return roi_color\n",
    "\n",
    "\n",
    "plt.imshow(image_detector_cropper('../Datasets/lionel_messi/_111066400_messi.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Individual Directories\n",
    "\n",
    "path_to_datasets = \"../Datasets/\"\n",
    "path_for_cropped_datasets = \"../Cropped_Datasets/\"\n",
    "\n",
    "image_directories = [] # Will Hold Names of Directories\n",
    "\n",
    "for directory in os.scandir(path_to_datasets) :\n",
    "\n",
    "    if directory.is_dir() :\n",
    "\n",
    "        image_directories.append(directory.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving to Cropped Datasets \n",
    "\n",
    "# Check if Directory Exists else Create\n",
    "if os.path.exists(path_for_cropped_datasets) :\n",
    "\n",
    "    shutil.rmtree(path_for_cropped_datasets)\n",
    "\n",
    "os.mkdir(path_for_cropped_datasets)\n",
    "\n",
    "\n",
    "# Create new individual directories\n",
    "\n",
    "cropped_img_directories = []\n",
    "sports_celeb_names = {}\n",
    "\n",
    "\n",
    "for directory in image_directories :\n",
    "\n",
    "    count = 1\n",
    "    celeb_name = directory.split('/')[-1]\n",
    "\n",
    "    for celeb_img in os.scandir(directory) :\n",
    "\n",
    "        cropped_img = image_detector_cropper(celeb_img.path)\n",
    "\n",
    "        if cropped_img is not None :\n",
    "\n",
    "            # Saved to Cropped Folder\n",
    "\n",
    "            cropped_img_folder = path_for_cropped_datasets + celeb_name\n",
    "\n",
    "            if not os.path.exists(cropped_img_folder) :\n",
    "\n",
    "                os.makedirs(cropped_img_folder)\n",
    "                cropped_img_directories.append(cropped_img_folder)\n",
    "\n",
    "\n",
    "                print(\"Generating Cropped Image Directory for : \", cropped_img_folder)\n",
    "\n",
    "            # Saving Cropped Images\n",
    "            croppped_filename = celeb_name + str(count) + \".png\"\n",
    "            cropped_filepath = cropped_img_folder + \"/\" + croppped_filename\n",
    "            cv2.imwrite(cropped_filepath, cropped_img)\n",
    "            sports_celeb_names.setdefault(celeb_name, [])\n",
    "            sports_celeb_names[celeb_name].append(cropped_filepath)\n",
    "\n",
    "            count  += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Using WaveletTranform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Perfrom Wavelet Tranformation\n",
    "def wavelet_transform(img, mode='haar', level=1):\n",
    "\n",
    "    img_arr = img\n",
    "\n",
    "    # Tranfrom Image \n",
    "    img_arr = cv2.cvtColor(img_arr, cv2.COLOR_RGB2GRAY)\n",
    "    img_arr = np.float32(img_arr)\n",
    "    img_arr /= 255\n",
    "\n",
    "    # Compute Coefficients\n",
    "    coefficients = pywt.wavedec2(img_arr, mode, level=level)\n",
    "    coefficients_H = list(coefficients)\n",
    "    coefficients_H[0] *= 0\n",
    "\n",
    "    # Reconstruct Image\n",
    "    img_arr_H = pywt.waverec2(coefficients_H, mode)\n",
    "    img_arr_H *= 255\n",
    "    img_arr_H = np.uint8(img_arr_H)\n",
    "\n",
    "\n",
    "    return img_arr_H \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_harred = wavelet_transform(cropped_img, 'db1', 5)\n",
    "plt.imshow(img_harred, cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Stacking Raw and Tranformed Images Vertically for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Numerical Classes for Celebrities\n",
    "\n",
    "classes = {}\n",
    "count = 1\n",
    "for celeb_name in sports_celeb_names.keys():\n",
    "\n",
    "    classes[celeb_name] = count\n",
    "    count +=1\n",
    "    \n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = []\n",
    "y =[]\n",
    "\n",
    "for celeb_name, img_files in sports_celeb_names.items():\n",
    "\n",
    "    for selected_img in img_files:\n",
    "\n",
    "        img = cv2.imread(selected_img)\n",
    "\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        scaled_raw_img = cv2.resize(img, (32, 32))\n",
    "\n",
    "        har_img = wavelet_transform(img, 'db1', 5)\n",
    "        scaled_har_img = cv2.resize(har_img, (32, 32))\n",
    "\n",
    "        stacked_img = np.vstack((scaled_raw_img.reshape(32*32*3, 1), scaled_har_img.reshape(32*32, 1)))\n",
    "\n",
    "        X.append(stacked_img)\n",
    "        y.append(celeb_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X).reshape(len(X), 4096).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pipeline = Pipeline([('scaler', StandardScaler()), ('svc', SVC(kernel= 'rbf', C=10))])\n",
    "my_pipeline.fit(X_train, y_train)\n",
    "print(\"Pipeline Accuracy = {accuracy}\".format(accuracy=(my_pipeline.score(X_test, y_test)*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, my_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Grid Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising Different Models\n",
    "\n",
    "model_params = {\n",
    "\n",
    "    'svm' : {\n",
    "        'model' : svm.SVC(gamma='auto', probability=True),\n",
    "        'params' : {\n",
    "            'svc__C' :[1, 10, 100, 1000],\n",
    "            'svc__kernel' : ['rbf', 'linear']\n",
    "        }\n",
    "    }, \n",
    "\n",
    "    'random_forest' : {\n",
    "        'model' : RandomForestClassifier(),\n",
    "        'params' : {\n",
    "            'randomforestclassifier__n_estimators' :[1, 5, 10]\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'logistic_regression' : {\n",
    "        'model' : LogisticRegression(solver= 'liblinear', multi_class='auto'),\n",
    "        'params' : {\n",
    "            'logisticregression__C' : [1, 5, 10]\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Models\n",
    "\n",
    "scores = []\n",
    "best_estimators = {}\n",
    "\n",
    "for algorithm, model_configs in model_params.items():\n",
    "\n",
    "    pipe_line = make_pipeline(StandardScaler(), model_configs['model'])\n",
    "    clf = GridSearchCV(pipe_line, model_configs['params'], cv=5, return_train_score=False)\n",
    "    clf.fit(X_train, y_train)\n",
    "    scores.append({\n",
    "        'model': algorithm,\n",
    "        'best_score' : clf.best_score_,\n",
    "        'best_parameters' : clf.best_params_\n",
    "    })\n",
    "\n",
    "    best_estimators[algorithm] = clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_df = pd.DataFrame(scores, columns=['model', 'best_score', 'best_parameters'])\n",
    "\n",
    "\n",
    "models_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_estimators)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing on Test Data\n",
    "\n",
    "best_estimators['svm'].score(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "best_estimators['random_forest'].score(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "best_estimators['logistic_regression'].score(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "# Plotting Confusion Matrix for Best Model Selection\n",
    "\n",
    "best_classifier = best_estimators['svm']\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, best_classifier.predict(X_test))\n",
    "\n",
    "plt.figure(figsize=(7,10))\n",
    "sb.heatmap(conf_mat, annot=True)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" best_classifier = best_estimators['logistic_regression']\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, best_classifier.predict(X_test))\n",
    "\n",
    "plt.figure(figsize=(7,10))\n",
    "sb.heatmap(conf_mat, annot=True)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual') \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Saving the Model\n",
    "\n",
    "\n",
    "# Save as a pickle file\n",
    "import joblib\n",
    "\n",
    "joblib.dump(best_classifier, 'model.pkl')\n",
    "\n",
    "\n",
    "# Saving classes \n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"classes_dict.json\", \"w\") as file :\n",
    "\n",
    "    file.write(json.dumps(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('cv': virtualenv)",
   "language": "python",
   "name": "python38064bitcvvirtualenvd7b733f42bb4414f9f59983f43ce56a0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}